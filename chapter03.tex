%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Разработка метода интеллектуального обнаружения клонов}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
В данном разделе приводится подробное описание предлагаемого интеллектуального метода обнаружения клонов. А именно, приводится общая структура процесса обнаружения клонов, описываются основные этапы предложенного метода, раскрываются задачи, решаемые на данных этапах, приводятся необходимые алгоритмы. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Общая схема алгоритма}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Предлагаемый метод состоит из следующих основных этапов:
\begin{enumerate}
\setlength\itemsep{0mm}
\item предобработка
\item преобразование
\item обучение НС
\item обнаружение клонов
\item постобработка
\end{enumerate}

В рамках данного метода, на первом этапе, исходный код программы представляется в виде синтаксического дерева. Далее из него извлекаются поддеревья, связанные с функциями или методами классов, которые затем преобразуются в последовательности токенов. Перед последующим преобразованием осуществляется фильтрация и нормализация поддеревьев. Иными словами, удаляются не интересующие нас элементы.

На следующем этапе производится создание векторного представления токенов. С его помощью производится последующее обучение нейронной сети и анализ исходного кода на наличие программных клонов.

Далее следует процесс обучения нейронной сети. В том случае, когда сеть уже обучена, на этом этапе будет производиться поиск клонов с помощью этой сети.

На последнем этапе из всех обнаруженных клонов формируются клоновые классы, которые, в последствии и будут представлены пользователю. Общая структура процесса обнаружения клонов представлена на рисунке~\ref{fig:struct}.

\begin{figure}[htbp]
\centering
\includegraphics[width=3in]{struct.png}
\caption{Общая структура процесса обнаружения клонов}
\label{fig:struct}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Предобработка}

На данном этапе, с помощью анализатора, основанного на части среды разработки (IDE Intellij IDEA community), производится представление исходного кода программы в виде AST. После чего, из полученного дерева выделяются только те поддеревья, которые относятся к методам или функциям. Такой поиск осуществляется посредством обхода в глубину всех вершин AST. В случае обнаружения вершины искомого типа, дальнейший спуск в данную ветку не осуществляется.

\nomenclature{IDE}{Integrated Development Environment}

Следующим этапом полученные поддеревья преобразуются в последовательности токенов путем того же обхода в глубину всех их вершин. После чего производится фильтрация этих последовательностей. Производимая фильтрация позволяет исключить влияние незначительных отличий фрагментов исходного кода друг от друга. В рамках данного подхода фильтрации подвергаются токены следующих типов:
\begin{itemize}
\setlength\itemsep{0mm}
\item комментарии различного типа
\item элементы форматирования
\item списки параметров
\end{itemize}

Обычно, для обнаружения клонов выше клонов I типа, необходимо производить анонимизацию токенов. Однако, в нашем методе, нейронная сеть будет сравнивать типы токенов. Из этого можно сделать вывод о ненадобности анонимизации. Таким образом, процесс фильтрации - последний процесс на этапе предобработки.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Преобразование}

На данном этапе производится преобразование полученных последовательностей токенов в их векторное представление. Данное преобразование производится с помощью модели Миколова - Word2Vec~\cite{word2vec}. Данная модель включает в себя набор алгоритмов расчета векторных представлений слов, предполагая семантическую близость слов используемых в похожих контекстах.

Рассматриваемая модель в своей работе использует нейронную сеть прямого распространения. В Word2Vec существуют два алгоритма обучения: CBOW (Continuous Bag of Words) и Skip-gram (рис. \ref{fig:word2vec}).

\nomenclature{CBOW}{Continuous Bag of Words}

CBOW и Skip-gram — это нейросетевые архитектуры, которые описывают, как именно нейросеть «учится» на данных и «запоминает» представления слов. Принципы у обоих архитектур разные. Принцип работы CBOW — предсказывание слова при данном контексте, а Skip-gram наоборот — предсказывается контекст при данном слове.

\begin{figure}[htbp]
\centering
\includegraphics[width=3.5in]{word2vec.png}
\caption{Подходы к обучению Word2Vec}
\label{fig:word2vec}
\end{figure}

В данном подходе используется алгоритм Skip-gram. Как уже говорилось ранее, цель обучения Skip-gram модели - найти такие представления слов, которые будут полезны для предсказания контекста в предложении или документе. Формально, при заданной последовательности обучающих слов \(w_1, w_2, w_3,...,w_T\) целью обучаемой модели является максимизация средней логарифмической вероятности (\ref{eq:word2vec_log})

\begin{equation}
\label{eq:word2vec_log}
\frac{1}{T}\sum_{t=1}^{T}\sum_{-c \leq j \leq c,j\neq0}\log p(w_{t+j}|w_t)
\end{equation}

, где \(c\) - размер обучающего контекста (который может быть функцией центрального слова \(w_t\))~\cite{word2vec}.

Таким образом, после выполнения данного этапа, были получены векторные представления для каждого токена.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Обучение нейронной сети}

В рамка предлагаемого метода обнаружение программных клонов осуществляется при помощи нейронных сетей. Для корректного использования сетей их необходимо обучить. В данной секции будет рассматриваться этап обучения нейронной сети.

Одним из самых важных элементов в обучении нейронных сетей является набор обучающих данных. В открытом доступе, для данной задачи, подходящих наборов данных практически нет. Использование результатов других утилит, в качестве набора данных, не позволит объективно оценить предлагаемый подход, так как такое решение приведет к копированию функциональности таких инструментов. В связи с чем было принято решение разработки простого «мутатора» для генерации обучающего набора данных.

Основная задача разрабатываемого «мутатора» заключается в генерации различных программных клонов I-III типа. При этом, сгенерированные клоны не обязаны быть корректными с точки зрения компилятора.

Общая структура работы «мутатора» приведена на рисунке~\ref{fig:mut_stages}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{mut_stages.png}
\caption{}
\label{fig:mut_stages}
\end{figure}
